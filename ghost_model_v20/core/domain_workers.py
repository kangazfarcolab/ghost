"""
Ghost v20 - Expanded Domain Workers
====================================
Code and Logic workers with MUCH larger datasets.
"""

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import random
import time
import sys
import os

base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.dirname(base_path))

from ghost_model_v20.core.unified_swarm import SharedBase, LoRAAdapter


# ============================================================
# WORKERS
# ============================================================

class CodeWorker(nn.Module):
    def __init__(self, base: SharedBase):
        super().__init__()
        self.base = base
        self.specialty = "code"
        self.lora = LoRAAdapter(base.dim, base.num_layers, rank=16)
        self.confidence = nn.Linear(base.dim, 1)
    
    def __call__(self, x):
        return self.base(x, self.lora)


class LogicWorker(nn.Module):
    def __init__(self, base: SharedBase):
        super().__init__()
        self.base = base
        self.specialty = "logic"
        self.lora = LoRAAdapter(base.dim, base.num_layers, rank=16)
        self.confidence = nn.Linear(base.dim, 1)
    
    def __call__(self, x):
        return self.base(x, self.lora)


# ============================================================
# EXPANDED CODE DATASET (100+ examples)
# ============================================================

CODE_PAIRS = [
    # Function definitions
    ("def hello():", " print('Hello')"),
    ("def add(a,b):", " return a+b"),
    ("def sub(a,b):", " return a-b"),
    ("def mul(a,b):", " return a*b"),
    ("def div(a,b):", " return a/b"),
    ("def max(a,b):", " return a if a>b else b"),
    ("def min(a,b):", " return a if a<b else b"),
    ("def abs(x):", " return x if x>0 else -x"),
    ("def square(x):", " return x*x"),
    ("def cube(x):", " return x*x*x"),
    ("def greet(name):", " print(f'Hello {name}')"),
    ("def double(x):", " return x*2"),
    ("def triple(x):", " return x*3"),
    ("def is_even(n):", " return n%2==0"),
    ("def is_odd(n):", " return n%2==1"),
    ("def is_positive(n):", " return n>0"),
    ("def is_negative(n):", " return n<0"),
    ("def is_zero(n):", " return n==0"),
    
    # Control flow
    ("if x>0:", " return True"),
    ("if x<0:", " return False"),
    ("if x==0:", " return None"),
    ("if x is None:", " raise ValueError"),
    ("if x>y:", " return x"),
    ("if x<y:", " return y"),
    ("if not x:", " x=0"),
    ("if x and y:", " return True"),
    ("if x or y:", " return True"),
    
    # Loops
    ("for i in range(10):", " print(i)"),
    ("for i in range(5):", " total+=i"),
    ("for x in list:", " print(x)"),
    ("for k,v in dict.items():", " print(k,v)"),
    ("for c in string:", " print(c)"),
    ("while True:", " break"),
    ("while x>0:", " x-=1"),
    ("while not done:", " process()"),
    
    # Classes
    ("class Dog:", " def bark(self): print('Woof')"),
    ("class Cat:", " def meow(self): print('Meow')"),
    ("class Car:", " def drive(self): pass"),
    ("class Point:", " def __init__(self,x,y): self.x,self.y=x,y"),
    ("class Stack:", " def push(self,x): self.items.append(x)"),
    ("class Queue:", " def enqueue(self,x): self.items.append(x)"),
    
    # Error handling
    ("try:", " result=risky()"),
    ("except:", " pass"),
    ("except Exception as e:", " print(e)"),
    ("finally:", " cleanup()"),
    ("raise ValueError:", " 'Invalid input'"),
    
    # Imports
    ("import os", ""),
    ("import sys", ""),
    ("import json", ""),
    ("import time", ""),
    ("import random", ""),
    ("from math import", " sqrt"),
    ("from typing import", " List"),
    
    # Data structures
    ("x=[1,2,3]", ""),
    ("y={'a':1}", ""),
    ("z=(1,2,3)", ""),
    ("s={1,2,3}", ""),
    ("d=dict()", ""),
    ("l=list()", ""),
    
    # List comprehensions
    ("[x for x in", " range(10)]"),
    ("[x*2 for x in", " range(5)]"),
    ("[x for x in items if", " x>0]"),
    
    # Lambda
    ("lambda x:", " x*2"),
    ("lambda x,y:", " x+y"),
    ("lambda:", " None"),
    
    # Returns
    ("return x+y", ""),
    ("return True", ""),
    ("return False", ""),
    ("return None", ""),
    ("return []", ""),
    ("return {}", ""),
    
    # Common patterns
    ("print('Hello", " World')"),
    ("print(f'Value:", " {x}')"),
    ("result=", " compute()"),
    ("self.", "x=x"),
    ("super().", "__init__()"),
    ("assert x>0,", " 'Must be positive'"),
    ("# TODO:", " implement"),
    ("# FIXME:", " bug here"),
    ("# NOTE:", " important"),
    
    # Type hints
    ("def foo(x:int)->", "int:"),
    ("def bar(s:str)->", "str:"),
    ("def baz(items:List)->", "List:"),
    
    # Async
    ("async def fetch():", " await get()"),
    ("await ", "response"),
    
    # Context managers
    ("with open(f):", " data=f.read()"),
    ("with lock:", " process()"),
]

# ============================================================
# EXPANDED LOGIC DATASET (100+ examples)
# ============================================================

LOGIC_PAIRS = [
    # Syllogisms
    ("All dogs bark. Rex is a dog. Does Rex bark?", "Yes"),
    ("All cats meow. Fluffy is a cat. Does Fluffy meow?", "Yes"),
    ("All birds fly. Tweety is a bird. Does Tweety fly?", "Yes"),
    ("All fish swim. Nemo is a fish. Does Nemo swim?", "Yes"),
    ("All cars drive. Tesla is a car. Does Tesla drive?", "Yes"),
    ("All humans think. John is human. Does John think?", "Yes"),
    ("No dogs fly. Rex is a dog. Does Rex fly?", "No"),
    ("No cats bark. Fluffy is a cat. Does Fluffy bark?", "No"),
    ("No fish walk. Nemo is a fish. Does Nemo walk?", "No"),
    
    # Some/Maybe
    ("Some cats are black. Felix is a cat. Is Felix black?", "Maybe"),
    ("Some dogs are big. Rex is a dog. Is Rex big?", "Maybe"),
    ("Some birds are blue. Tweety is a bird. Is Tweety blue?", "Maybe"),
    
    # Conditionals
    ("If A then B. A is true. Is B true?", "Yes"),
    ("If A then B. B is false. Is A true?", "No"),
    ("If A then B. B is true. Is A true?", "Maybe"),
    ("If A then B. A is false. Is B true?", "Maybe"),
    ("If rain then wet. It rains. Ground?", "Wet"),
    ("If hot then melt. It's hot. Ice?", "Melts"),
    ("If cold then freeze. It's cold. Water?", "Freezes"),
    
    # Modus tollens
    ("P implies Q. Not Q. P?", "No"),
    ("A implies B. Not B. A?", "No"),
    ("X implies Y. Not Y. X?", "No"),
    
    # Modus ponens
    ("P implies Q. P. Q?", "Yes"),
    ("A implies B. A. B?", "Yes"),
    ("X implies Y. X. Y?", "Yes"),
    
    # Chains
    ("A->B. B->C. A. C?", "Yes"),
    ("P->Q. Q->R. P. R?", "Yes"),
    ("X->Y. Y->Z. X. Z?", "Yes"),
    
    # OR logic
    ("A or B. Not A. B?", "Yes"),
    ("X or Y. Not X. Y?", "Yes"),
    ("P or Q. Not P. Q?", "Yes"),
    ("A or B. Both true?", "Maybe"),
    
    # AND logic
    ("A and B. A true. B?", "Maybe"),
    ("A and B required. A false. Success?", "No"),
    ("X and Y. X false. Result?", "False"),
    
    # Comparisons
    ("1<2?", "Yes"),
    ("5>3?", "Yes"),
    ("2=2?", "Yes"),
    ("3>5?", "No"),
    ("10<5?", "No"),
    ("Bigger: 100 or 99?", "100"),
    ("Bigger: 5 or 10?", "10"),
    ("Smaller: 3 or 7?", "3"),
    ("Equal: 5 and 5?", "Yes"),
    ("Equal: 3 and 4?", "No"),
    
    # Math verification
    ("2+2=4?", "Yes"),
    ("3+3=6?", "Yes"),
    ("5+5=10?", "Yes"),
    ("2+2=5?", "No"),
    ("3*3=9?", "Yes"),
    ("4*4=16?", "Yes"),
    ("10/2=5?", "Yes"),
    
    # Negations
    ("Not true?", "False"),
    ("Not false?", "True"),
    ("Not (A and B) = ?", "Not A or Not B"),
    ("Not (A or B) = ?", "Not A and Not B"),
    
    # True/False questions
    ("Is 5 prime?", "Yes"),
    ("Is 4 prime?", "No"),
    ("Is 7 even?", "No"),
    ("Is 8 even?", "Yes"),
    ("Is 0 positive?", "No"),
    ("Is -5 negative?", "Yes"),
    
    # Set theory
    ("A subset of B. B subset of C. A subset of C?", "Yes"),
    ("X in A. A in B. X in B?", "Maybe"),
    ("Empty set subset of all?", "Yes"),
    
    # Exclusive or
    ("A xor B. A true. B?", "False"),
    ("A xor B. Both true?", "No"),
    ("A xor B. Neither true?", "No"),
    
    # Biconditional
    ("A iff B. A true. B?", "True"),
    ("A iff B. B false. A?", "False"),
    
    # Quantifiers
    ("For all x, P(x). Specific a. P(a)?", "Yes"),
    ("Exists x, P(x). Specific a. P(a)?", "Maybe"),
    ("No x, P(x). Specific a. P(a)?", "No"),
    
    # Contradiction
    ("A and not A possible?", "No"),
    ("Can X be both true and false?", "No"),
    ("P and not P?", "Impossible"),
    
    # Tautology
    ("A or not A?", "Always true"),
    ("P or not P?", "Tautology"),
    ("True or anything?", "True"),
    ("False and anything?", "False"),
]


def encode(text, max_len=48):
    tokens = [ord(c) for c in text[:max_len]]
    return tokens + [0] * (max_len - len(tokens))


def create_batch(pairs, batch_size=16):
    samples = random.choices(pairs, k=batch_size)
    inputs, targets = [], []
    for q, a in samples:
        text = f"{q}{a}"
        tokens = encode(text)
        inputs.append(tokens[:-1])
        targets.append(tokens[1:])
    return mx.array(inputs, dtype=mx.int32), mx.array(targets, dtype=mx.int32)


def ce_loss(logits, targets):
    logits_flat = logits.reshape(-1, 256)
    targets_flat = targets.reshape(-1)
    log_probs = mx.log(mx.softmax(logits_flat, axis=-1) + 1e-10)
    return -mx.mean(mx.take_along_axis(log_probs, targets_flat[:, None], axis=1))


def train_worker(base, worker, pairs, steps=4000, lr=1e-3):
    opt = optim.AdamW(learning_rate=lr)
    
    for step in range(steps):
        x, y = create_batch(pairs, batch_size=16)
        
        def loss_fn(params):
            worker.lora.update(params)
            logits = worker(x)
            return ce_loss(logits, y)
        
        loss, grads = mx.value_and_grad(loss_fn)(worker.lora.trainable_parameters())
        opt.update(worker.lora, grads)
        mx.eval(worker.lora.parameters())
        
        if step % 1000 == 0 or step == steps - 1:
            print(f"   Step {step:4d} | Loss: {float(loss):.4f}")


def generate(worker, prompt: str, max_tokens: int = 20) -> str:
    tokens = [ord(c) for c in prompt]
    for _ in range(max_tokens):
        x = mx.array([tokens], dtype=mx.int32)
        logits = worker(x)
        next_token = int(mx.argmax(logits[0, -1]).item())
        if next_token == 0 or next_token == 10:
            break
        tokens.append(next_token)
    return "".join(chr(t) if 32 <= t < 127 else "" for t in tokens[len(prompt):]).strip()


def test_accuracy(worker, pairs, max_check=50):
    correct = 0
    samples = random.sample(pairs, min(max_check, len(pairs)))
    for q, a in samples:
        pred = generate(worker, q)
        # Check if prediction starts with expected
        if pred.lower().startswith(a.lower()[:3]) or a.lower() in pred.lower():
            correct += 1
    return correct / len(samples)


def main():
    print("=" * 60)
    print("     GHOST v20 - EXPANDED DOMAIN WORKERS")
    print("=" * 60)
    
    base = SharedBase(dim=128, num_layers=4)
    mx.eval(base.parameters())
    
    code_worker = CodeWorker(base)
    logic_worker = LogicWorker(base)
    mx.eval(code_worker.lora.parameters())
    mx.eval(logic_worker.lora.parameters())
    
    print(f"\nüì¶ Base: {base.count_params():,} params")
    print(f"   Code dataset: {len(CODE_PAIRS)} examples")
    print(f"   Logic dataset: {len(LOGIC_PAIRS)} examples")
    
    print("\nüìö TRAINING")
    print("-" * 60)
    
    start = time.perf_counter()
    
    print("\n   Training Code Worker (4000 steps):")
    train_worker(base, code_worker, CODE_PAIRS, steps=4000)
    
    print("\n   Training Logic Worker (4000 steps):")
    train_worker(base, logic_worker, LOGIC_PAIRS, steps=4000)
    
    train_time = time.perf_counter() - start
    print(f"\n‚è±Ô∏è Total training: {train_time:.1f}s")
    
    # Test
    print("\nüß™ TESTING")
    print("-" * 60)
    
    print("\nüíª Code Worker:")
    code_tests = [
        ("def hello():", " print"),
        ("if x>0:", " return"),
        ("for i in range(10):", " print"),
        ("lambda x:", " x"),
        ("class Dog:", " def"),
    ]
    code_correct = 0
    for prompt, expected in code_tests:
        result = generate(code_worker, prompt)
        status = "‚úÖ" if expected in result else "‚ùå"
        print(f"   {prompt} ‚Üí '{result[:25]}' {status}")
        if expected in result:
            code_correct += 1
    
    print("\nüß† Logic Worker:")
    logic_tests = [
        ("2+2=4?", "Yes"),
        ("5>3?", "Yes"),
        ("If A then B. A is true. B?", "Yes"),
        ("All dogs bark. Rex is a dog. Does Rex bark?", "Yes"),
        ("Bigger: 100 or 99?", "100"),
    ]
    logic_correct = 0
    for prompt, expected in logic_tests:
        result = generate(logic_worker, prompt)
        status = "‚úÖ" if expected.lower() in result.lower() else "‚ùå"
        print(f"   {prompt} ‚Üí '{result[:15]}' {status}")
        if expected.lower() in result.lower():
            logic_correct += 1
    
    print("\n" + "=" * 60)
    print("              SUMMARY")
    print("=" * 60)
    print(f"   Code accuracy: {code_correct}/{len(code_tests)}")
    print(f"   Logic accuracy: {logic_correct}/{len(logic_tests)}")
    print(f"   Training time: {train_time:.1f}s")
    print("=" * 60)


if __name__ == "__main__":
    main()
